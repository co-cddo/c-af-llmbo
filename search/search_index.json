{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLMbo - Large Language model batch operations","text":"<p>A low bar to using batch inference in AWS bedrock.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#llmbo","title":"LLMbo","text":""},{"location":"api/#llmbo.llmbo.BatchInferer","title":"<code>BatchInferer</code>","text":"<p>A class to manage batch inference jobs using AWS Bedrock.</p> <p>This class handles the creation, monitoring, and retrieval of batch inference jobs for large-scale model invocations using AWS Bedrock service.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name/ID of the AWS Bedrock model to use</p> required <code>bucket_name</code> <code>str</code> <p>The S3 bucket name for storing input/output data</p> required <code>job_name</code> <code>str</code> <p>A unique name for the batch inference job</p> required <code>role_arn</code> <code>str</code> <p>The AWS IAM role ARN with necessary permissions</p> required <code>time_out_duration_hours</code> <code>int</code> <p>Maximum job runtime in hours. Defaults to 24.</p> <code>24</code> <p>Attributes:</p> Name Type Description <code>job_arn</code> <code>str</code> <p>The ARN of the created batch inference job</p> <code>results</code> <code>List[dict]</code> <p>The results of the batch inference job. Available after job completion.</p> <code>manifest</code> <code>Manifest</code> <p>Job execution statistics. Available after job completion.</p> <code>job_status</code> <code>str</code> <p>Current status of the batch job. One of VALID_FINISHED_STATUSES.</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>class BatchInferer:\n    \"\"\"A class to manage batch inference jobs using AWS Bedrock.\n\n    This class handles the creation, monitoring, and retrieval of batch inference jobs\n    for large-scale model invocations using AWS Bedrock service.\n\n    Args:\n        model_name (str): The name/ID of the AWS Bedrock model to use\n        bucket_name (str): The S3 bucket name for storing input/output data\n        job_name (str): A unique name for the batch inference job\n        role_arn (str): The AWS IAM role ARN with necessary permissions\n        time_out_duration_hours (int, optional): Maximum job runtime in hours. Defaults to 24.\n\n    Attributes:\n        job_arn (str): The ARN of the created batch inference job\n        results (List[dict]): The results of the batch inference job. Available after job completion.\n        manifest (Manifest): Job execution statistics. Available after job completion.\n        job_status (str): Current status of the batch job. One of VALID_FINISHED_STATUSES.\n    \"\"\"\n\n    logger = logging.getLogger(f\"{__name__}.BatchInferer\")\n\n    def __init__(\n        self,\n        model_name: str,  # this should be an enum...\n        bucket_name: str,\n        region: str,\n        job_name: str,\n        role_arn: str,\n        time_out_duration_hours: int = 24,\n    ):\n        \"\"\"Initialize a BatchInferer for AWS Bedrock batch processing.\n\n        Creates a configured batch inference manager that handles the end-to-end process\n        of submitting and managing batch jobs on AWS Bedrock.\n\n        Args:\n            model_name (str): The AWS Bedrock model identifier (e.g., 'anthropic.claude-3-haiku-20240307-v1:0')\n            bucket_name (str): Name of the S3 bucket for storing job inputs and outputs\n            region (str): The region containing the llm to call, must match the bucket region\n            job_name (str): Unique identifier for this batch job. Used in file naming.\n            role_arn (str): AWS IAM role ARN with permissions for Bedrock and S3 access\n            time_out_duration_hours (int, optional): Maximum runtime for the batch job. Defaults to 24 hours.\n\n        Raises:\n            KeyError: If AWS_PROFILE environment variable is not set\n            ValueError: If the provided role_arn doesn't exist or is invalid\n\n        Example:\n        ```python\n            &gt;&gt;&gt; bi = BatchInferer(\n                    model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n                    bucket_name=\"my-inference-bucket\",\n                    job_name=\"batch-job-2024-01-01\",\n                    role_arn=\"arn:aws:iam::123456789012:role/BedrockBatchRole\"\n                )\n        ```\n\n        Note:\n            - Requires valid AWS credentials and configuration\n            - The S3 bucket must exist and be accessible via the provided role\n            - Job name will be used to create unique file names for inputs and outputs\n        \"\"\"\n\n        self.logger.info(\"Intialising BatchInferer\")\n        # model parameters\n        self.model_name = model_name\n        self.time_out_duration_hours = time_out_duration_hours\n\n        self.session: boto3.Session = boto3.Session()\n\n        # file/bucket parameters\n        self._check_bucket(bucket_name, region)\n        self.bucket_name = bucket_name\n        self.bucket_uri = \"s3://\" + bucket_name\n        self.job_name = job_name or \"batch_inference_\" + uuid4.uuid4()[:6]\n        self.file_name = job_name + \".jsonl\"\n        self.output_file_name = None\n        self.manifest_file_name = None\n\n        self.check_for_profile()\n        self._check_arn(role_arn)\n        self.role_arn = role_arn\n        self.region = region\n\n        self.client: boto3.client = self.session.client(\"bedrock\", region_name=region)\n\n        # internal state - created by the class later.\n        self.job_arn = None\n        self.job_status = None\n        self.results = None\n        self.manifest = None\n        self.requests = None\n\n        self.logger.info(\"Initialized BatchInferer\")\n\n    @property\n    def unique_id_from_arn(self):\n        if not self.job_arn:\n            self.logger.error(\"Job ARN not set\")\n            raise ValueError(\"Job ARN not set\")\n        return self.job_arn.split(\"/\")[-1]\n\n    def check_for_profile(self):\n        if not os.getenv(\"AWS_PROFILE\"):\n            self.logger.error(\"AWS_PROFILE environment variable not set\")\n            raise KeyError(\"AWS_PROFILE environment variable not set\")\n\n    @staticmethod\n    def _read_jsonl(file_path):\n        data = []\n        with open(file_path, \"r\") as file:\n            for line in file:\n                data.append(json.loads(line.strip()))\n        return data\n\n    def _get_bucket_location(self, bucket_name: str) -&gt; str:\n        \"\"\"\n        get the location of the s3 bucket\n\n        Args:\n            bucket_name (str): the name of a bucket\n\n        Raises:\n            ValueError: If the bucket is not accessible\n\n        Returns:\n            str: a region, e.g. \"eu-west-2\"\n        \"\"\"\n        try:\n            s3_client = self.session.client(\"s3\")\n            response = s3_client.get_bucket_location(Bucket=bucket_name)\n\n            if response:\n                region = response[\"LocationConstraint\"]\n                # aws returns None if the region is us-east-1 otherwise it returns the region\n                return region if region else \"us-east-1\"\n        except ClientError as e:\n            self.logger.error(f\"Bucket {bucket_name} is not accessible: {e}\")\n            raise ValueError(f\"Bucket {bucket_name} is not accessible\")\n\n    def _check_bucket(self, bucket_name: str, region: str) -&gt; None:\n        \"\"\"\n        Validate if the bucket_name provided exists\n\n        Args:\n            bucket_name (str): the name of a bucket\n            region (str): the name of a region\n\n        Raises:\n            ValueError: If the bucket is not accessible\n            ValueError: If the bucket is not in the same region as the LLM.\n        \"\"\"\n        try:\n            s3_client = self.session.client(\"s3\")\n            s3_client.head_bucket(Bucket=bucket_name)\n        except ClientError as e:\n            self.logger.error(f\"Bucket {bucket_name} is not accessible: {e}\")\n            raise ValueError(f\"Bucket {bucket_name} is not accessible\")\n\n        if (bucket_region := self._get_bucket_location(bucket_name)) != region:\n            self.logger.error(\n                f\"Bucket {bucket_name} is not located in the same region [{region}] as the llm [{bucket_region}]\"\n            )\n            raise ValueError(\n                f\"Bucket {bucket_name} is not located in the same region [{region}] as the llm [{bucket_region}]\"\n            )\n\n    def _check_arn(self, role_arn: str) -&gt; bool:\n        \"\"\"Validate if an IAM role exists and is accessible.\n\n        Attempts to retrieve the IAM role using the provided ARN to verify its\n        existence and accessibility.\n\n        Args:\n            role_arn (str): The AWS ARN of the IAM role to check.\n                Format: 'arn:aws:iam::&lt;account-id&gt;:role/&lt;role-name&gt;'\n\n        Returns:\n            bool: True if the role exists and is accessible.\n\n        Raises:\n            ValueError: If the role does not exist.\n        ClientError: If there are AWS API issues unrelated to role existence.\"\"\"\n\n        if not role_arn.startswith(\"arn:aws:iam::\"):\n            self.logger.error(\"Invalid ARN format\")\n            raise ValueError(\"Invalid ARN format\")\n\n        # Extract the role name from the ARN\n        role_name = role_arn.split(\"/\")[-1]\n\n        iam_client = self.session.client(\"iam\")\n\n        try:\n            # Try to get the role\n            iam_client.get_role(RoleName=role_name)\n            self.logger.info(f\"Role '{role_name}' exists.\")\n            return True\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n                self.logger.error(f\"Role '{role_name}' does not exist.\")\n                raise ValueError(f\"Role '{role_name}' does not exist.\")\n            else:\n                raise e\n\n    def prepare_requests(self, inputs: Dict[str, ModelInput]) -&gt; None:\n        \"\"\"Prepare batch inference requests from a dictionary of model inputs.\n\n        Formats model inputs into the required JSONL structure for AWS Bedrock batch processing.\n        Each request is formatted as:\n            {\n                \"recordId\": str,\n                \"modelInput\": dict\n            }\n\n        Args:\n            inputs (Dict[str, ModelInput]): Dictionary mapping record IDs to their corresponding\n                ModelInput configurations. The record IDs will be used to track results.\n\n        Raises:\n            ValueError: If len(inputs) &lt; 100, as AWS Bedrock requires minimum batch size of 100\n\n        Example:\n            &gt;&gt;&gt; inputs = {\n            ...     \"001\": ModelInput(\n            ...         messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n            ...         temperature=0.7\n            ...     ),\n            ...     \"002\": ModelInput(\n            ...         messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n            ...         temperature=0.7\n            ...     )\n            ... }\n            &gt;&gt;&gt; bi.prepare_requests(inputs)\n\n        Note:\n            - This method must be called before push_requests_to_s3()\n            - The prepared requests are stored in self.requests\n            - Each ModelInput is converted to a dict using its to_dict() method\n        \"\"\"\n        # maybe a data class conforming to this???\n        #\n\n        self.logger.info(f\"Preparing {len(inputs)} requests\")\n        if len(inputs) &lt; 100:\n            self.logger.error(f\"Minimum Batch Size is 100, {len(inputs)} given.\")\n            raise ValueError(f\"Minimum Batch Size is 100, {len(inputs)} given.\")\n\n        self.requests = [\n            {\n                \"recordId\": id,\n                \"modelInput\": model_input.to_dict(),\n            }\n            for id, model_input in inputs.items()\n        ]\n\n    def _write_requests_locally(self) -&gt; None:\n        \"\"\"Write batch inference requests to a local JSONL file.\n\n        Creates or overwrites a local JSONL file containing the prepared inference\n        requests. Each line contains a JSON object with recordId and modelInput.\n\n        Raises:\n            IOError: If unable to write to the file\n            AttributeError: If called before prepare_requests()\n\n        Note:\n            - File is named according to self.file_name\n            - Internal method used by push_requests_to_s3()\n            - Will overwrite existing files with the same name\n        \"\"\"\n        self.logger.info(f\"Writing {len(self.requests)} requests to {self.file_name}\")\n        with open(self.file_name, \"w\") as file:\n            for record in self.requests:\n                file.write(json.dumps(record) + \"\\n\")\n\n    def push_requests_to_s3(self) -&gt; Dict[str, Any]:\n        \"\"\"Upload batch inference requests to S3.\n\n        Writes the prepared requests to a local JSONL file and uploads it to the\n        configured S3 bucket in the 'input/' prefix.\n\n        Returns:\n            dict: The S3 upload response from boto3\n\n        Raises:\n            IOError: If local file operations fail\n            ClientError: If S3 upload fails\n            AttributeError: If called before prepare_requests()\n\n        Note:\n            - Creates/overwrites files both locally and in S3\n            - S3 path: {bucket_name}/input/{job_name}.jsonl\n            - Sets Content-Type to 'application/json'\n        \"\"\"\n        # do I want to write this file locally? - maybe stream it or write it to\n        # temp file instead\n        self._write_requests_locally()\n        s3_client = self.session.client(\"s3\")\n        self.logger.info(f\"Pushing {len(self.requests)} requests to {self.bucket_name}\")\n        response = s3_client.upload_file(\n            Filename=self.file_name,\n            Bucket=self.bucket_name,\n            Key=f\"input/{self.file_name}\",\n            ExtraArgs={\"ContentType\": \"application/json\"},\n        )\n        return response\n\n    def create(self) -&gt; Dict[str, Any]:\n        \"\"\"Create a new batch inference job in AWS Bedrock.\n\n        Initializes a new model invocation job using the configured parameters\n        and uploaded input data.\n\n        Returns:\n            dict: The complete response from the create_model_invocation_job API call\n\n        Raises:\n            RuntimeError: If job creation fails\n            ClientError: For AWS API errors\n            ValueError: If required configurations are missing\n\n        Note:\n            - Sets self.job_arn on successful creation\n            - Input data must be uploaded to S3 before calling this method\n            - Job will timeout after self.time_out_duration_hours\n        \"\"\"\n        if self.requests:\n            self.logger.info(f\"Creating job {self.job_name}\")\n            response = self.client.create_model_invocation_job(\n                jobName=self.job_name,\n                roleArn=self.role_arn,\n                clientRequestToken=\"string\",\n                modelId=self.model_name,\n                inputDataConfig={\n                    \"s3InputDataConfig\": {\n                        \"s3InputFormat\": \"JSONL\",\n                        \"s3Uri\": f\"{self.bucket_uri}/input/{self.file_name}\",\n                    }\n                },\n                outputDataConfig={\n                    \"s3OutputDataConfig\": {\n                        \"s3Uri\": f\"{self.bucket_uri}/output/\",\n                    }\n                },\n                timeoutDurationInHours=self.time_out_duration_hours,\n                tags=[{\"key\": \"bedrock_batch_inference\", \"value\": self.job_name}],\n            )\n\n            if response:\n                response_status = response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n                if response_status == 200:\n                    self.logger.info(f\"Job {self.job_name} created successfully\")\n                    self.logger.info(f\"Assigned jobArn: {response['jobArn']}\")\n                    self.job_arn = response[\"jobArn\"]\n                    return response\n                else:\n                    self.logger.error(\n                        f\"There was an error creating the job {self.job_name}, non 200 response from bedrock\"\n                    )\n                    raise RuntimeError(\n                        f\"There was an error creating the job {self.job_name}, non 200 response from bedrock\"\n                    )\n            else:\n                self.logger.error(\n                    \"There was an error creating the job, no response from bedrock\"\n                )\n                raise RuntimeError(\n                    \"There was an error creating the job, no response from bedrock\"\n                )\n        else:\n            self.logger.error(\"There were no prepared requests\")\n            raise AttributeError(\"There were no prepared requests\")\n\n    def download_results(self) -&gt; None:\n        \"\"\"Download batch inference results from S3.\n\n        Retrieves both the results and manifest files from S3 once the job\n        has completed. Files are downloaded to:\n            - {job_name}_out.jsonl: Contains model outputs\n            - {job_name}_manifest.jsonl: Contains job statistics\n\n        Raises:\n            ClientError: For S3 download failures\n            ValueError: If job hasn't completed or job_arn isn't set\n\n        Note:\n            - Only downloads if job status is in VALID_FINISHED_STATUSES\n            - Files are downloaded to current working directory\n            - Existing files will be overwritten\n            - Call check_complete() first to ensure job is finished\n        \"\"\"\n        if self.check_complete() in VALID_FINISHED_STATUSES:\n            file_name_, ext = os.path.splitext(self.file_name)\n            self.output_file_name = f\"{file_name_}_out{ext}\"\n            self.manifest_file_name = f\"{file_name_}_manifest{ext}\"\n            self.logger.info(\n                f\"Job:{self.job_arn} Complete. Downloading results from {self.bucket_name}\"\n            )\n            s3_client = self.session.client(\"s3\")\n            s3_client.download_file(\n                Bucket=self.bucket_name,\n                Key=f\"output/{self.unique_id_from_arn}/{self.file_name}.out\",\n                Filename=self.output_file_name,\n            )\n            self.logger.info(f\"Downloaded results file to {self.output_file_name}\")\n\n            s3_client.download_file(\n                Bucket=self.bucket_name,\n                Key=f\"output/{self.unique_id_from_arn}/manifest.json.out\",\n                Filename=self.manifest_file_name,\n            )\n            self.logger.info(f\"Downloaded manifest file to {self.manifest_file_name}\")\n        else:\n            self.logger.info(\n                f\"Job:{self.job_arn} was not marked one of {VALID_FINISHED_STATUSES}, could not download.\"\n            )\n\n    def load_results(self) -&gt; None:\n        \"\"\"Load batch inference results and manifest from local files.\n\n        Reads and parses the output files downloaded from S3, populating:\n            - self.results: List of inference results from the output JSONL file\n            - self.manifest: Statistics about the job execution (total records, success/error counts, etc.)\n\n        The method expects two files to exist locally:\n            - {job_name}_out.jsonl: Contains the model outputs\n            - {job_name}_manifest.jsonl: Contains execution statistics\n\n        Raises:\n            FileExistsError: If either the results or manifest files are not found locally\n\n        Note:\n            - Must call download_results() before calling this method\n            - The manifest provides useful metrics like success rate and token counts\n        \"\"\"\n        if os.path.isfile(self.output_file_name) and os.path.isfile(\n            self.manifest_file_name\n        ):\n            self.results = self._read_jsonl(self.output_file_name)\n            self.manifest = Manifest(**self._read_jsonl(self.manifest_file_name)[0])\n        else:\n            self.logger.error(\n                \"Result files do not exist, you may need to call .download_results() first.\"\n            )\n            raise FileExistsError(\n                \"Result files do not exist, you may need to call .download_results() first.\"\n            )\n\n    def cancel_batch(self) -&gt; None:\n        \"\"\"Cancel a running batch inference job.\n\n        Attempts to stop the currently running batch inference job identified by self.job_arn.\n\n        Returns:\n            None\n\n        Raises:\n            RuntimeError: If the job cancellation request fails\n            ValueError: If no job_arn is set (i.e., no job has been created)\n        \"\"\"\n\n        if not self.job_arn:\n            self.logger.error(\"No job_arn set - no job to cancel\")\n            raise ValueError(\"No job_arn set - no job to cancel\")\n\n        response = self.client.stop_model_invocation_job(jobIdentifier=self.job_arn)\n\n        if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n            self.logger.info(\n                f\"Job {self.job_name} with id={self.job_arn} was cancelled\"\n            )\n            self.job_status = \"Stopped\"\n        else:\n            self.logger.error(\n                f\"Failed to cancel job {self.job_name}. Status: {response['ResponseMetadata']['HTTPStatusCode']}\"\n            )\n            raise RuntimeError(f\"Failed to cancel job {self.job_name}\")\n\n    def check_complete(self) -&gt; Optional[str]:\n        \"\"\"Check if the batch inference job has completed.\n\n        Returns:\n            Optional[str]: The job status if completed (one of VALID_FINISHED_STATUSES), None otherwise\n        \"\"\"\n        if self.job_status not in VALID_FINISHED_STATUSES:\n            self.logger.info(f\"Checking status of job {self.job_arn}\")\n            response = self.client.get_model_invocation_job(jobIdentifier=self.job_arn)\n\n            self.job_status = response[\"status\"]\n            self.logger.info(f\"Job status is {self.job_status}\")\n\n            if self.job_status in VALID_FINISHED_STATUSES:\n                return self.job_status\n            return None\n        else:\n            self.logger.info(f\"Job {self.job_arn} is already {self.job_status}\")\n            return self.job_status\n\n    def poll_progress(self, poll_interval_seconds: int = 60) -&gt; bool:\n        \"\"\"Polls the progress of a job\n\n        Args:\n            poll_interval_seconds (int, optional): Number of seconds between checks. Defaults to 60.\n\n        Returns:\n            bool: True if job is complete.\n        \"\"\"\n        self.logger.info(f\"Polling for progress every {poll_interval_seconds} seconds\")\n        while not self.check_complete():\n            time.sleep(poll_interval_seconds)\n        return True\n\n    def auto(self, inputs: Dict[str, ModelInput]) -&gt; dict[str, ModelInput]:\n        \"\"\"Execute the complete batch inference workflow automatically.\n\n        This method combines the preparation, execution, monitoring, and result retrieval\n        steps into a single operation.\n\n        Args:\n            inputs (Dict[str, ModelInput]): Dictionary of record IDs mapped to their ModelInput configurations\n\n        Returns:\n            List[dict]: The results of the batch inference job\n        \"\"\"\n        self.prepare_requests(inputs)\n        self.push_requests_to_s3()\n        self.create()\n        self.poll_progress(10 * 60)\n        self.download_results()\n        self.load_results()\n        return self.results\n\n    @classmethod\n    def recover_details_from_job_arn(\n        cls, job_arn: str, region_name: str\n    ) -&gt; \"BatchInferer\":\n        \"\"\"Recover a BatchInferer instance from an existing job ARN.\n\n        Used to reconstruct a BatchInferer object when the original Python process\n        has terminated but the AWS job is still running or complete.\n\n        Args:\n            job_arn: The AWS ARN of the existing batch inference job\n\n        Returns:\n            BatchInferer: A configured instance with the job's details\n\n        Raises:\n            ValueError: If the job cannot be found or response is invalid\n\n        Example:\n            &gt;&gt;&gt; job_arn = \"arn:aws:bedrock:region:account:job/xyz123\"\n            &gt;&gt;&gt; bi = BatchInferer.recover_details_from_job_arn(job_arn)\n            &gt;&gt;&gt; bi.check_complete()\n            'Completed'\n        \"\"\"\n\n        cls.logger.info(f\"Attempting to Recover BatchInferer from {job_arn}\")\n        if not job_arn.startswith(\"arn:aws:bedrock:\"):\n            cls.logger.error(f\"Invalid Bedrock ARN format: {job_arn}\")\n            raise ValueError(f\"Invalid Bedrock ARN format: {job_arn}\")\n        session = boto3.Session()\n        client = session(\"bedrock\", region_name=region_name)\n\n        try:\n            response = client.get_model_invocation_job(jobIdentifier=job_arn)\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n                cls.logger.error(f\"Job not found: {job_arn}\")\n                raise ValueError(f\"Job not found: {job_arn}\") from e\n            cls.logger.error(f\"AWS API error: {str(e)}\")\n            raise RuntimeError(f\"AWS API error: {str(e)}\") from e\n\n        if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n            cls.logger.error(\n                f\"Unexpected response status: {response['ResponseMetadata']['HTTPStatusCode']}\"\n            )\n            raise RuntimeError(\n                f\"Unexpected response status: {response['ResponseMetadata']['HTTPStatusCode']}\"\n            )\n\n        try:\n            # Extract required parameters from response\n            job_name = response[\"jobName\"]\n            model_id = response[\"modelId\"]\n            bucket_name = response[\"inputDataConfig\"][\"s3InputDataConfig\"][\n                \"s3Uri\"\n            ].split(\"/\")[2]\n            role_arn = response[\"roleArn\"]\n\n            # Validate required files exist\n            input_file = f\"{job_name}.jsonl\"\n            if not os.path.exists(input_file):\n                cls.logger.error(f\"Required input file not found: {input_file}\")\n                raise FileNotFoundError(f\"Required input file not found: {input_file}\")\n\n            requests = cls._read_jsonl(input_file)\n\n            bi = cls(\n                model_name=model_id,\n                job_name=job_name,\n                bucket_name=bucket_name,\n                role_arn=role_arn,\n            )\n            bi.job_arn = job_arn\n            bi.requests = requests\n            bi.job_status = response[\"status\"]\n\n            return bi\n\n        except (KeyError, IndexError) as e:\n            cls.logger.error(f\"Invalid job response format: {str(e)}\")\n            raise ValueError(f\"Invalid job response format: {str(e)}\") from e\n        except Exception as e:\n            cls.logger.error(f\"Failed to recover job details: {str(e)}\")\n            raise RuntimeError(f\"Failed to recover job details: {str(e)}\") from e\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.__init__","title":"<code>__init__(model_name, bucket_name, region, job_name, role_arn, time_out_duration_hours=24)</code>","text":"<p>Initialize a BatchInferer for AWS Bedrock batch processing.</p> <p>Creates a configured batch inference manager that handles the end-to-end process of submitting and managing batch jobs on AWS Bedrock.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The AWS Bedrock model identifier (e.g., 'anthropic.claude-3-haiku-20240307-v1:0')</p> required <code>bucket_name</code> <code>str</code> <p>Name of the S3 bucket for storing job inputs and outputs</p> required <code>region</code> <code>str</code> <p>The region containing the llm to call, must match the bucket region</p> required <code>job_name</code> <code>str</code> <p>Unique identifier for this batch job. Used in file naming.</p> required <code>role_arn</code> <code>str</code> <p>AWS IAM role ARN with permissions for Bedrock and S3 access</p> required <code>time_out_duration_hours</code> <code>int</code> <p>Maximum runtime for the batch job. Defaults to 24 hours.</p> <code>24</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If AWS_PROFILE environment variable is not set</p> <code>ValueError</code> <p>If the provided role_arn doesn't exist or is invalid</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; bi = BatchInferer(\n            model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n            bucket_name=\"my-inference-bucket\",\n            job_name=\"batch-job-2024-01-01\",\n            role_arn=\"arn:aws:iam::123456789012:role/BedrockBatchRole\"\n        )\n</code></pre> Note <ul> <li>Requires valid AWS credentials and configuration</li> <li>The S3 bucket must exist and be accessible via the provided role</li> <li>Job name will be used to create unique file names for inputs and outputs</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,  # this should be an enum...\n    bucket_name: str,\n    region: str,\n    job_name: str,\n    role_arn: str,\n    time_out_duration_hours: int = 24,\n):\n    \"\"\"Initialize a BatchInferer for AWS Bedrock batch processing.\n\n    Creates a configured batch inference manager that handles the end-to-end process\n    of submitting and managing batch jobs on AWS Bedrock.\n\n    Args:\n        model_name (str): The AWS Bedrock model identifier (e.g., 'anthropic.claude-3-haiku-20240307-v1:0')\n        bucket_name (str): Name of the S3 bucket for storing job inputs and outputs\n        region (str): The region containing the llm to call, must match the bucket region\n        job_name (str): Unique identifier for this batch job. Used in file naming.\n        role_arn (str): AWS IAM role ARN with permissions for Bedrock and S3 access\n        time_out_duration_hours (int, optional): Maximum runtime for the batch job. Defaults to 24 hours.\n\n    Raises:\n        KeyError: If AWS_PROFILE environment variable is not set\n        ValueError: If the provided role_arn doesn't exist or is invalid\n\n    Example:\n    ```python\n        &gt;&gt;&gt; bi = BatchInferer(\n                model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n                bucket_name=\"my-inference-bucket\",\n                job_name=\"batch-job-2024-01-01\",\n                role_arn=\"arn:aws:iam::123456789012:role/BedrockBatchRole\"\n            )\n    ```\n\n    Note:\n        - Requires valid AWS credentials and configuration\n        - The S3 bucket must exist and be accessible via the provided role\n        - Job name will be used to create unique file names for inputs and outputs\n    \"\"\"\n\n    self.logger.info(\"Intialising BatchInferer\")\n    # model parameters\n    self.model_name = model_name\n    self.time_out_duration_hours = time_out_duration_hours\n\n    self.session: boto3.Session = boto3.Session()\n\n    # file/bucket parameters\n    self._check_bucket(bucket_name, region)\n    self.bucket_name = bucket_name\n    self.bucket_uri = \"s3://\" + bucket_name\n    self.job_name = job_name or \"batch_inference_\" + uuid4.uuid4()[:6]\n    self.file_name = job_name + \".jsonl\"\n    self.output_file_name = None\n    self.manifest_file_name = None\n\n    self.check_for_profile()\n    self._check_arn(role_arn)\n    self.role_arn = role_arn\n    self.region = region\n\n    self.client: boto3.client = self.session.client(\"bedrock\", region_name=region)\n\n    # internal state - created by the class later.\n    self.job_arn = None\n    self.job_status = None\n    self.results = None\n    self.manifest = None\n    self.requests = None\n\n    self.logger.info(\"Initialized BatchInferer\")\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.auto","title":"<code>auto(inputs)</code>","text":"<p>Execute the complete batch inference workflow automatically.</p> <p>This method combines the preparation, execution, monitoring, and result retrieval steps into a single operation.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, ModelInput]</code> <p>Dictionary of record IDs mapped to their ModelInput configurations</p> required <p>Returns:</p> Type Description <code>dict[str, ModelInput]</code> <p>List[dict]: The results of the batch inference job</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def auto(self, inputs: Dict[str, ModelInput]) -&gt; dict[str, ModelInput]:\n    \"\"\"Execute the complete batch inference workflow automatically.\n\n    This method combines the preparation, execution, monitoring, and result retrieval\n    steps into a single operation.\n\n    Args:\n        inputs (Dict[str, ModelInput]): Dictionary of record IDs mapped to their ModelInput configurations\n\n    Returns:\n        List[dict]: The results of the batch inference job\n    \"\"\"\n    self.prepare_requests(inputs)\n    self.push_requests_to_s3()\n    self.create()\n    self.poll_progress(10 * 60)\n    self.download_results()\n    self.load_results()\n    return self.results\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.cancel_batch","title":"<code>cancel_batch()</code>","text":"<p>Cancel a running batch inference job.</p> <p>Attempts to stop the currently running batch inference job identified by self.job_arn.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the job cancellation request fails</p> <code>ValueError</code> <p>If no job_arn is set (i.e., no job has been created)</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def cancel_batch(self) -&gt; None:\n    \"\"\"Cancel a running batch inference job.\n\n    Attempts to stop the currently running batch inference job identified by self.job_arn.\n\n    Returns:\n        None\n\n    Raises:\n        RuntimeError: If the job cancellation request fails\n        ValueError: If no job_arn is set (i.e., no job has been created)\n    \"\"\"\n\n    if not self.job_arn:\n        self.logger.error(\"No job_arn set - no job to cancel\")\n        raise ValueError(\"No job_arn set - no job to cancel\")\n\n    response = self.client.stop_model_invocation_job(jobIdentifier=self.job_arn)\n\n    if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n        self.logger.info(\n            f\"Job {self.job_name} with id={self.job_arn} was cancelled\"\n        )\n        self.job_status = \"Stopped\"\n    else:\n        self.logger.error(\n            f\"Failed to cancel job {self.job_name}. Status: {response['ResponseMetadata']['HTTPStatusCode']}\"\n        )\n        raise RuntimeError(f\"Failed to cancel job {self.job_name}\")\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.check_complete","title":"<code>check_complete()</code>","text":"<p>Check if the batch inference job has completed.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The job status if completed (one of VALID_FINISHED_STATUSES), None otherwise</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def check_complete(self) -&gt; Optional[str]:\n    \"\"\"Check if the batch inference job has completed.\n\n    Returns:\n        Optional[str]: The job status if completed (one of VALID_FINISHED_STATUSES), None otherwise\n    \"\"\"\n    if self.job_status not in VALID_FINISHED_STATUSES:\n        self.logger.info(f\"Checking status of job {self.job_arn}\")\n        response = self.client.get_model_invocation_job(jobIdentifier=self.job_arn)\n\n        self.job_status = response[\"status\"]\n        self.logger.info(f\"Job status is {self.job_status}\")\n\n        if self.job_status in VALID_FINISHED_STATUSES:\n            return self.job_status\n        return None\n    else:\n        self.logger.info(f\"Job {self.job_arn} is already {self.job_status}\")\n        return self.job_status\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.create","title":"<code>create()</code>","text":"<p>Create a new batch inference job in AWS Bedrock.</p> <p>Initializes a new model invocation job using the configured parameters and uploaded input data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The complete response from the create_model_invocation_job API call</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If job creation fails</p> <code>ClientError</code> <p>For AWS API errors</p> <code>ValueError</code> <p>If required configurations are missing</p> Note <ul> <li>Sets self.job_arn on successful creation</li> <li>Input data must be uploaded to S3 before calling this method</li> <li>Job will timeout after self.time_out_duration_hours</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def create(self) -&gt; Dict[str, Any]:\n    \"\"\"Create a new batch inference job in AWS Bedrock.\n\n    Initializes a new model invocation job using the configured parameters\n    and uploaded input data.\n\n    Returns:\n        dict: The complete response from the create_model_invocation_job API call\n\n    Raises:\n        RuntimeError: If job creation fails\n        ClientError: For AWS API errors\n        ValueError: If required configurations are missing\n\n    Note:\n        - Sets self.job_arn on successful creation\n        - Input data must be uploaded to S3 before calling this method\n        - Job will timeout after self.time_out_duration_hours\n    \"\"\"\n    if self.requests:\n        self.logger.info(f\"Creating job {self.job_name}\")\n        response = self.client.create_model_invocation_job(\n            jobName=self.job_name,\n            roleArn=self.role_arn,\n            clientRequestToken=\"string\",\n            modelId=self.model_name,\n            inputDataConfig={\n                \"s3InputDataConfig\": {\n                    \"s3InputFormat\": \"JSONL\",\n                    \"s3Uri\": f\"{self.bucket_uri}/input/{self.file_name}\",\n                }\n            },\n            outputDataConfig={\n                \"s3OutputDataConfig\": {\n                    \"s3Uri\": f\"{self.bucket_uri}/output/\",\n                }\n            },\n            timeoutDurationInHours=self.time_out_duration_hours,\n            tags=[{\"key\": \"bedrock_batch_inference\", \"value\": self.job_name}],\n        )\n\n        if response:\n            response_status = response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n            if response_status == 200:\n                self.logger.info(f\"Job {self.job_name} created successfully\")\n                self.logger.info(f\"Assigned jobArn: {response['jobArn']}\")\n                self.job_arn = response[\"jobArn\"]\n                return response\n            else:\n                self.logger.error(\n                    f\"There was an error creating the job {self.job_name}, non 200 response from bedrock\"\n                )\n                raise RuntimeError(\n                    f\"There was an error creating the job {self.job_name}, non 200 response from bedrock\"\n                )\n        else:\n            self.logger.error(\n                \"There was an error creating the job, no response from bedrock\"\n            )\n            raise RuntimeError(\n                \"There was an error creating the job, no response from bedrock\"\n            )\n    else:\n        self.logger.error(\"There were no prepared requests\")\n        raise AttributeError(\"There were no prepared requests\")\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.download_results","title":"<code>download_results()</code>","text":"<p>Download batch inference results from S3.</p> <p>Retrieves both the results and manifest files from S3 once the job has completed. Files are downloaded to:     - {job_name}_out.jsonl: Contains model outputs     - {job_name}_manifest.jsonl: Contains job statistics</p> <p>Raises:</p> Type Description <code>ClientError</code> <p>For S3 download failures</p> <code>ValueError</code> <p>If job hasn't completed or job_arn isn't set</p> Note <ul> <li>Only downloads if job status is in VALID_FINISHED_STATUSES</li> <li>Files are downloaded to current working directory</li> <li>Existing files will be overwritten</li> <li>Call check_complete() first to ensure job is finished</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def download_results(self) -&gt; None:\n    \"\"\"Download batch inference results from S3.\n\n    Retrieves both the results and manifest files from S3 once the job\n    has completed. Files are downloaded to:\n        - {job_name}_out.jsonl: Contains model outputs\n        - {job_name}_manifest.jsonl: Contains job statistics\n\n    Raises:\n        ClientError: For S3 download failures\n        ValueError: If job hasn't completed or job_arn isn't set\n\n    Note:\n        - Only downloads if job status is in VALID_FINISHED_STATUSES\n        - Files are downloaded to current working directory\n        - Existing files will be overwritten\n        - Call check_complete() first to ensure job is finished\n    \"\"\"\n    if self.check_complete() in VALID_FINISHED_STATUSES:\n        file_name_, ext = os.path.splitext(self.file_name)\n        self.output_file_name = f\"{file_name_}_out{ext}\"\n        self.manifest_file_name = f\"{file_name_}_manifest{ext}\"\n        self.logger.info(\n            f\"Job:{self.job_arn} Complete. Downloading results from {self.bucket_name}\"\n        )\n        s3_client = self.session.client(\"s3\")\n        s3_client.download_file(\n            Bucket=self.bucket_name,\n            Key=f\"output/{self.unique_id_from_arn}/{self.file_name}.out\",\n            Filename=self.output_file_name,\n        )\n        self.logger.info(f\"Downloaded results file to {self.output_file_name}\")\n\n        s3_client.download_file(\n            Bucket=self.bucket_name,\n            Key=f\"output/{self.unique_id_from_arn}/manifest.json.out\",\n            Filename=self.manifest_file_name,\n        )\n        self.logger.info(f\"Downloaded manifest file to {self.manifest_file_name}\")\n    else:\n        self.logger.info(\n            f\"Job:{self.job_arn} was not marked one of {VALID_FINISHED_STATUSES}, could not download.\"\n        )\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.load_results","title":"<code>load_results()</code>","text":"<p>Load batch inference results and manifest from local files.</p> <p>Reads and parses the output files downloaded from S3, populating:     - self.results: List of inference results from the output JSONL file     - self.manifest: Statistics about the job execution (total records, success/error counts, etc.)</p> The method expects two files to exist locally <ul> <li>{job_name}_out.jsonl: Contains the model outputs</li> <li>{job_name}_manifest.jsonl: Contains execution statistics</li> </ul> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If either the results or manifest files are not found locally</p> Note <ul> <li>Must call download_results() before calling this method</li> <li>The manifest provides useful metrics like success rate and token counts</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def load_results(self) -&gt; None:\n    \"\"\"Load batch inference results and manifest from local files.\n\n    Reads and parses the output files downloaded from S3, populating:\n        - self.results: List of inference results from the output JSONL file\n        - self.manifest: Statistics about the job execution (total records, success/error counts, etc.)\n\n    The method expects two files to exist locally:\n        - {job_name}_out.jsonl: Contains the model outputs\n        - {job_name}_manifest.jsonl: Contains execution statistics\n\n    Raises:\n        FileExistsError: If either the results or manifest files are not found locally\n\n    Note:\n        - Must call download_results() before calling this method\n        - The manifest provides useful metrics like success rate and token counts\n    \"\"\"\n    if os.path.isfile(self.output_file_name) and os.path.isfile(\n        self.manifest_file_name\n    ):\n        self.results = self._read_jsonl(self.output_file_name)\n        self.manifest = Manifest(**self._read_jsonl(self.manifest_file_name)[0])\n    else:\n        self.logger.error(\n            \"Result files do not exist, you may need to call .download_results() first.\"\n        )\n        raise FileExistsError(\n            \"Result files do not exist, you may need to call .download_results() first.\"\n        )\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.poll_progress","title":"<code>poll_progress(poll_interval_seconds=60)</code>","text":"<p>Polls the progress of a job</p> <p>Parameters:</p> Name Type Description Default <code>poll_interval_seconds</code> <code>int</code> <p>Number of seconds between checks. Defaults to 60.</p> <code>60</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if job is complete.</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def poll_progress(self, poll_interval_seconds: int = 60) -&gt; bool:\n    \"\"\"Polls the progress of a job\n\n    Args:\n        poll_interval_seconds (int, optional): Number of seconds between checks. Defaults to 60.\n\n    Returns:\n        bool: True if job is complete.\n    \"\"\"\n    self.logger.info(f\"Polling for progress every {poll_interval_seconds} seconds\")\n    while not self.check_complete():\n        time.sleep(poll_interval_seconds)\n    return True\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.prepare_requests","title":"<code>prepare_requests(inputs)</code>","text":"<p>Prepare batch inference requests from a dictionary of model inputs.</p> <p>Formats model inputs into the required JSONL structure for AWS Bedrock batch processing. Each request is formatted as:     {         \"recordId\": str,         \"modelInput\": dict     }</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, ModelInput]</code> <p>Dictionary mapping record IDs to their corresponding ModelInput configurations. The record IDs will be used to track results.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If len(inputs) &lt; 100, as AWS Bedrock requires minimum batch size of 100</p> Example <p>inputs = { ...     \"001\": ModelInput( ...         messages=[{\"role\": \"user\", \"content\": \"Hello\"}], ...         temperature=0.7 ...     ), ...     \"002\": ModelInput( ...         messages=[{\"role\": \"user\", \"content\": \"Hi\"}], ...         temperature=0.7 ...     ) ... } bi.prepare_requests(inputs)</p> Note <ul> <li>This method must be called before push_requests_to_s3()</li> <li>The prepared requests are stored in self.requests</li> <li>Each ModelInput is converted to a dict using its to_dict() method</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def prepare_requests(self, inputs: Dict[str, ModelInput]) -&gt; None:\n    \"\"\"Prepare batch inference requests from a dictionary of model inputs.\n\n    Formats model inputs into the required JSONL structure for AWS Bedrock batch processing.\n    Each request is formatted as:\n        {\n            \"recordId\": str,\n            \"modelInput\": dict\n        }\n\n    Args:\n        inputs (Dict[str, ModelInput]): Dictionary mapping record IDs to their corresponding\n            ModelInput configurations. The record IDs will be used to track results.\n\n    Raises:\n        ValueError: If len(inputs) &lt; 100, as AWS Bedrock requires minimum batch size of 100\n\n    Example:\n        &gt;&gt;&gt; inputs = {\n        ...     \"001\": ModelInput(\n        ...         messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n        ...         temperature=0.7\n        ...     ),\n        ...     \"002\": ModelInput(\n        ...         messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n        ...         temperature=0.7\n        ...     )\n        ... }\n        &gt;&gt;&gt; bi.prepare_requests(inputs)\n\n    Note:\n        - This method must be called before push_requests_to_s3()\n        - The prepared requests are stored in self.requests\n        - Each ModelInput is converted to a dict using its to_dict() method\n    \"\"\"\n    # maybe a data class conforming to this???\n    #\n\n    self.logger.info(f\"Preparing {len(inputs)} requests\")\n    if len(inputs) &lt; 100:\n        self.logger.error(f\"Minimum Batch Size is 100, {len(inputs)} given.\")\n        raise ValueError(f\"Minimum Batch Size is 100, {len(inputs)} given.\")\n\n    self.requests = [\n        {\n            \"recordId\": id,\n            \"modelInput\": model_input.to_dict(),\n        }\n        for id, model_input in inputs.items()\n    ]\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.push_requests_to_s3","title":"<code>push_requests_to_s3()</code>","text":"<p>Upload batch inference requests to S3.</p> <p>Writes the prepared requests to a local JSONL file and uploads it to the configured S3 bucket in the 'input/' prefix.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>The S3 upload response from boto3</p> <p>Raises:</p> Type Description <code>IOError</code> <p>If local file operations fail</p> <code>ClientError</code> <p>If S3 upload fails</p> <code>AttributeError</code> <p>If called before prepare_requests()</p> Note <ul> <li>Creates/overwrites files both locally and in S3</li> <li>S3 path: {bucket_name}/input/{job_name}.jsonl</li> <li>Sets Content-Type to 'application/json'</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def push_requests_to_s3(self) -&gt; Dict[str, Any]:\n    \"\"\"Upload batch inference requests to S3.\n\n    Writes the prepared requests to a local JSONL file and uploads it to the\n    configured S3 bucket in the 'input/' prefix.\n\n    Returns:\n        dict: The S3 upload response from boto3\n\n    Raises:\n        IOError: If local file operations fail\n        ClientError: If S3 upload fails\n        AttributeError: If called before prepare_requests()\n\n    Note:\n        - Creates/overwrites files both locally and in S3\n        - S3 path: {bucket_name}/input/{job_name}.jsonl\n        - Sets Content-Type to 'application/json'\n    \"\"\"\n    # do I want to write this file locally? - maybe stream it or write it to\n    # temp file instead\n    self._write_requests_locally()\n    s3_client = self.session.client(\"s3\")\n    self.logger.info(f\"Pushing {len(self.requests)} requests to {self.bucket_name}\")\n    response = s3_client.upload_file(\n        Filename=self.file_name,\n        Bucket=self.bucket_name,\n        Key=f\"input/{self.file_name}\",\n        ExtraArgs={\"ContentType\": \"application/json\"},\n    )\n    return response\n</code></pre>"},{"location":"api/#llmbo.llmbo.BatchInferer.recover_details_from_job_arn","title":"<code>recover_details_from_job_arn(job_arn, region_name)</code>  <code>classmethod</code>","text":"<p>Recover a BatchInferer instance from an existing job ARN.</p> <p>Used to reconstruct a BatchInferer object when the original Python process has terminated but the AWS job is still running or complete.</p> <p>Parameters:</p> Name Type Description Default <code>job_arn</code> <code>str</code> <p>The AWS ARN of the existing batch inference job</p> required <p>Returns:</p> Name Type Description <code>BatchInferer</code> <code>BatchInferer</code> <p>A configured instance with the job's details</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the job cannot be found or response is invalid</p> Example <p>job_arn = \"arn:aws:bedrock:region:account:job/xyz123\" bi = BatchInferer.recover_details_from_job_arn(job_arn) bi.check_complete() 'Completed'</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>@classmethod\ndef recover_details_from_job_arn(\n    cls, job_arn: str, region_name: str\n) -&gt; \"BatchInferer\":\n    \"\"\"Recover a BatchInferer instance from an existing job ARN.\n\n    Used to reconstruct a BatchInferer object when the original Python process\n    has terminated but the AWS job is still running or complete.\n\n    Args:\n        job_arn: The AWS ARN of the existing batch inference job\n\n    Returns:\n        BatchInferer: A configured instance with the job's details\n\n    Raises:\n        ValueError: If the job cannot be found or response is invalid\n\n    Example:\n        &gt;&gt;&gt; job_arn = \"arn:aws:bedrock:region:account:job/xyz123\"\n        &gt;&gt;&gt; bi = BatchInferer.recover_details_from_job_arn(job_arn)\n        &gt;&gt;&gt; bi.check_complete()\n        'Completed'\n    \"\"\"\n\n    cls.logger.info(f\"Attempting to Recover BatchInferer from {job_arn}\")\n    if not job_arn.startswith(\"arn:aws:bedrock:\"):\n        cls.logger.error(f\"Invalid Bedrock ARN format: {job_arn}\")\n        raise ValueError(f\"Invalid Bedrock ARN format: {job_arn}\")\n    session = boto3.Session()\n    client = session(\"bedrock\", region_name=region_name)\n\n    try:\n        response = client.get_model_invocation_job(jobIdentifier=job_arn)\n    except ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n            cls.logger.error(f\"Job not found: {job_arn}\")\n            raise ValueError(f\"Job not found: {job_arn}\") from e\n        cls.logger.error(f\"AWS API error: {str(e)}\")\n        raise RuntimeError(f\"AWS API error: {str(e)}\") from e\n\n    if response[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200:\n        cls.logger.error(\n            f\"Unexpected response status: {response['ResponseMetadata']['HTTPStatusCode']}\"\n        )\n        raise RuntimeError(\n            f\"Unexpected response status: {response['ResponseMetadata']['HTTPStatusCode']}\"\n        )\n\n    try:\n        # Extract required parameters from response\n        job_name = response[\"jobName\"]\n        model_id = response[\"modelId\"]\n        bucket_name = response[\"inputDataConfig\"][\"s3InputDataConfig\"][\n            \"s3Uri\"\n        ].split(\"/\")[2]\n        role_arn = response[\"roleArn\"]\n\n        # Validate required files exist\n        input_file = f\"{job_name}.jsonl\"\n        if not os.path.exists(input_file):\n            cls.logger.error(f\"Required input file not found: {input_file}\")\n            raise FileNotFoundError(f\"Required input file not found: {input_file}\")\n\n        requests = cls._read_jsonl(input_file)\n\n        bi = cls(\n            model_name=model_id,\n            job_name=job_name,\n            bucket_name=bucket_name,\n            role_arn=role_arn,\n        )\n        bi.job_arn = job_arn\n        bi.requests = requests\n        bi.job_status = response[\"status\"]\n\n        return bi\n\n    except (KeyError, IndexError) as e:\n        cls.logger.error(f\"Invalid job response format: {str(e)}\")\n        raise ValueError(f\"Invalid job response format: {str(e)}\") from e\n    except Exception as e:\n        cls.logger.error(f\"Failed to recover job details: {str(e)}\")\n        raise RuntimeError(f\"Failed to recover job details: {str(e)}\") from e\n</code></pre>"},{"location":"api/#llmbo.llmbo.ModelInput","title":"<code>ModelInput</code>  <code>dataclass</code>","text":"<p>Configuration class for AWS Bedrock model inputs.</p> <p>This class defines the structure and parameters for model invocation requests following AWS Bedrock's expected format.</p> <p>See https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[dict]</code> <p>List of message objects with role and content</p> <code>anthropic_version</code> <code>str</code> <p>Version string for Anthropic models</p> <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens in the response</p> <code>system</code> <code>Optional[str]</code> <p>System message for the model</p> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>Custom stop sequences</p> <code>temperature</code> <code>Optional[float]</code> <p>Sampling temperature</p> <code>top_p</code> <code>Optional[float]</code> <p>Nucleus sampling parameter</p> <code>top_k</code> <code>Optional[int]</code> <p>Top-k sampling parameter</p> <code>tools</code> <code>Optional[List[dict]]</code> <p>Tool definitions for structured outputs</p> <code>tool_choice</code> <code>Optional[ToolChoice]</code> <p>Tool selection configuration</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>@dataclass\nclass ModelInput:\n    \"\"\"Configuration class for AWS Bedrock model inputs.\n\n    This class defines the structure and parameters for model invocation requests\n    following AWS Bedrock's expected format.\n\n    See https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html\n\n    Attributes:\n        messages (List[dict]): List of message objects with role and content\n        anthropic_version (str): Version string for Anthropic models\n        max_tokens (int): Maximum number of tokens in the response\n        system (Optional[str]): System message for the model\n        stop_sequences (Optional[List[str]]): Custom stop sequences\n        temperature (Optional[float]): Sampling temperature\n        top_p (Optional[float]): Nucleus sampling parameter\n        top_k (Optional[int]): Top-k sampling parameter\n        tools (Optional[List[dict]]): Tool definitions for structured outputs\n        tool_choice (Optional[ToolChoice]): Tool selection configuration\n    \"\"\"\n\n    # These are required\n    messages: List[dict]\n    anthropic_version: str = \"bedrock-2023-05-31\"\n    max_tokens: int = 2000\n\n    system: Optional[str] = None\n    stop_sequences: Optional[List[str]] | None = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    top_k: Optional[int] = None\n\n    tools: Optional[List[dict]] | None = None\n    tool_choice: Optional[ToolChoice] = None\n\n    def to_dict(self):\n        result = {k: v for k, v in self.__dict__.items() if v is not None}\n        if self.tool_choice:\n            result[\"tool_choice\"] = self.tool_choice.__dict__\n        return result\n\n    def to_json(self):\n        return json.dumps(self.to_dict())\n</code></pre>"},{"location":"api/#llmbo.llmbo.StructuredBatchInferer","title":"<code>StructuredBatchInferer</code>","text":"<p>               Bases: <code>BatchInferer</code></p> <p>A specialized BatchInferer that enforces structured outputs using Pydantic models.</p> <p>Inspired by the instructor package, see: https://python.useinstructor.com/ This class extends BatchInferer to add schema validation and structured output handling using Pydantic models.</p> <p>Parameters:</p> Name Type Description Default <code>output_model</code> <code>BaseModel</code> <p>A Pydantic model defining the expected output structure</p> required <code>model_name</code> <code>str</code> <p>The name/ID of the AWS Bedrock model to use</p> required <code>bucket_name</code> <code>str</code> <p>The S3 bucket name for storing input/output data</p> required <code>job_name</code> <code>str</code> <p>A unique name for the batch inference job</p> required <code>role_arn</code> <code>str</code> <p>The AWS IAM role ARN with necessary permissions</p> required Source code in <code>src/llmbo/llmbo.py</code> <pre><code>class StructuredBatchInferer(BatchInferer):\n    \"\"\"A specialized BatchInferer that enforces structured outputs using Pydantic models.\n\n    Inspired by the instructor package, see: https://python.useinstructor.com/\n    This class extends BatchInferer to add schema validation and structured output\n    handling using Pydantic models.\n\n    Args:\n        output_model (BaseModel): A Pydantic model defining the expected output structure\n        model_name (str): The name/ID of the AWS Bedrock model to use\n        bucket_name (str): The S3 bucket name for storing input/output data\n        job_name (str): A unique name for the batch inference job\n        role_arn (str): The AWS IAM role ARN with necessary permissions\n    \"\"\"\n\n    logger = logging.getLogger(f\"{__name__}.StructuredBatchInferer\")\n\n    def __init__(\n        self,\n        output_model: BaseModel,\n        model_name: str,  # this should be an enum...\n        region: str,\n        bucket_name: str,\n        job_name: str,\n        role_arn: str,\n    ):\n        \"\"\"Initialize a StructuredBatchInferer for schema-validated batch processing.\n\n        Creates a batch inference manager that enforces structured outputs using\n        a Pydantic model schema. Automatically configures the model to use tools\n        for enforcing the output structure.\n\n        Args:\n            output_model (BaseModel): Pydantic model class defining the expected output structure\n            model_name (str): The AWS Bedrock model identifier\n            bucket_name (str): Name of the S3 bucket for storing job inputs and outputs\n            region (str): Region of the LLM must match the bucket\n            job_name (str): Unique identifier for this batch job\n            role_arn (str): AWS IAM role ARN with permissions for Bedrock and S3 access\n\n        Raises:\n            KeyError: If AWS_PROFILE environment variable is not set\n            ValueError: If the provided role_arn doesn't exist or is invalid\n\n        Example:\n            &gt;&gt;&gt; class PersonInfo(BaseModel):\n            ...     name: str\n            ...     age: int\n            ...\n            &gt;&gt;&gt; sbi = StructuredBatchInferer(\n            ...     output_model=PersonInfo,\n            ...     model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n            ...     bucket_name=\"my-inference-bucket\",\n            ...     job_name=\"structured-batch-2024\",\n            ...     role_arn=\"arn:aws:iam::123456789012:role/BedrockBatchRole\"\n            ... )\n\n        Note:\n            - Converts the Pydantic model into a tool definition for the LLM\n            - All results will be validated against the provided schema\n            - Failed schema validations will raise errors during result processing\n            - Inherits all base BatchInferer functionality\n        \"\"\"\n        self.output_model = output_model\n        self.tool = self._build_tool()\n        self.logger.info(\n            f\"Initialized StructuredBatchInferer with {output_model.__name__} schema\"\n        )\n        super().__init__(model_name, bucket_name, job_name, role_arn, region)\n\n    def _build_tool(self) -&gt; dict:\n        \"\"\"Convert a Pydantic model into a tool definition for the model.\n\n        Returns:\n            dict: A tool description containing name, description, and input schema\n        \"\"\"\n        return {\n            \"name\": self.output_model.__name__,\n            \"description\": self.output_model.__doc__ or \"please fill in the schema\",\n            \"input_schema\": self.output_model.model_json_schema(),\n        }\n\n    def prepare_requests(self, inputs: Dict[str, ModelInput]):\n        \"\"\"Prepare structured batch inference requests with tool configurations.\n\n        Extends the base preparation by adding tool definitions and tool choice\n        parameters to each ModelInput. The tool definition is derived from the\n        Pydantic output_model specified during initialization.\n\n        Args:\n            inputs (Dict[str, ModelInput]): Dictionary mapping record IDs to their corresponding\n                ModelInput configurations. The record IDs will be used to track results.\n\n        Raises:\n            ValueError: If len(inputs) &lt; 100, as AWS Bedrock requires minimum batch size of 100\n\n        Example:\n            &gt;&gt;&gt; class PersonInfo(BaseModel):\n            ...     name: str\n            ...     age: int\n            &gt;&gt;&gt; sbi = StructuredBatchInferer(output_model=PersonInfo, ...)\n            &gt;&gt;&gt; inputs = {\n            ...     \"001\": ModelInput(\n            ...         messages=[{\"role\": \"user\", \"content\": \"John is 25 years old\"}],\n            ...     )\n            ... }\n            &gt;&gt;&gt; sbi.prepare_requests(inputs)\n\n        Note:\n            - Automatically adds the output_model schema as a tool definition\n            - Sets tool_choice to force use of the defined schema\n            - Original ModelInputs are modified to include tool configurations\n        \"\"\"\n        with_tools = {\n            id: self._add_tool_to_model_input(model_input)\n            for id, model_input in inputs.items()\n        }\n        super().prepare_requests(with_tools)\n\n    def _add_tool_to_model_input(self, model_input: ModelInput) -&gt; ModelInput:\n        \"\"\"Add tool definition and configuration to a ModelInput instance.\n\n        Updates the ModelInput by:\n            1. Adding the Pydantic model schema as a tool definition\n            2. Setting tool_choice to force use of this specific tool\n\n        Args:\n            model_input (ModelInput): The original model input configuration\n\n        Returns:\n            ModelInput: The modified model input with tool configurations added\n        \"\"\"\n        self.logger.info(f\"Adding tool {self.tool['name']} to model input\")\n        model_input.tools = [self.tool]\n        model_input.tool_choice = ToolChoice(\n            type=\"tool\", name=self.output_model.__name__\n        )\n        return model_input\n\n    def load_results(self):\n        \"\"\"Load and validate batch inference results against the output schema.\n\n        Reads the output files downloaded from S3 and validates each result against\n        the Pydantic output_model specified during initialization. Populates:\n            - self.results: Raw inference results from the output JSONL file\n            - self.manifest: Statistics about the job execution\n            - self.instances: List of validated Pydantic model instances\n\n        Raises:\n            FileExistsError: If either the results or manifest files are not found locally\n            ValueError: If any result fails schema validation or tool use validation\n\n        Note:\n            - Must call download_results() before calling this method\n            - All results must conform to the specified output_model schema\n            - Results must show successful tool use\n        \"\"\"\n        super().load_results()\n        self.instances = [\n            self.validate_result(result[\"modelOutput\"]) for result in self.results\n        ]\n\n    def validate_result(\n        self,\n        result: dict,\n    ) -&gt; BaseModel:\n        \"\"\"Validate and parse a single model output against the schema.\n\n        Checks that the model used the specified tool correctly and validates\n        the output against the Pydantic model schema.\n\n        Args:\n            result (dict): The raw model output containing content and metadata\n\n        Returns:\n            BaseModel: An instance of the output_model containing the validated data\n\n        Raises:\n            ValueError: If:\n                - Model didn't use the tool\n                - Multiple tool uses were found\n                - Output doesn't match schema\n            TypeError: If output data types don't match schema\n\n        Example:\n            &gt;&gt;&gt; result = {\"stop_reason\": \"tool_use\",\n            ...          \"content\": [{\"type\": \"tool_use\",\n            ...                      \"input\": {\"name\": \"John\", \"age\": 30}}]}\n            &gt;&gt;&gt; instance = sbi.validate_result(result)\n            &gt;&gt;&gt; print(instance.name)\n            'John'\n        \"\"\"\n        if not result[\"stop_reason\"] == \"tool_use\":\n            self.logger.error(\"Model did not use tool\")\n            raise ValueError(\"Model did not use tool\")\n        if not len(result[\"content\"]) == 1:\n            self.logger.error(\"Multiple instances of tool use per execution\")\n            raise ValueError(\"Multiple instances of tool use per execution\")\n        if result[\"content\"][0][\"type\"] == \"tool_use\":\n            try:\n                output = self.output_model(**result[\"content\"][0][\"input\"])\n                return output\n            except TypeError as e:\n                self.logger.error(f\"Could not validate output {e}\")\n                raise ValueError(f\"Could not validate output {e}\")\n</code></pre>"},{"location":"api/#llmbo.llmbo.StructuredBatchInferer.__init__","title":"<code>__init__(output_model, model_name, region, bucket_name, job_name, role_arn)</code>","text":"<p>Initialize a StructuredBatchInferer for schema-validated batch processing.</p> <p>Creates a batch inference manager that enforces structured outputs using a Pydantic model schema. Automatically configures the model to use tools for enforcing the output structure.</p> <p>Parameters:</p> Name Type Description Default <code>output_model</code> <code>BaseModel</code> <p>Pydantic model class defining the expected output structure</p> required <code>model_name</code> <code>str</code> <p>The AWS Bedrock model identifier</p> required <code>bucket_name</code> <code>str</code> <p>Name of the S3 bucket for storing job inputs and outputs</p> required <code>region</code> <code>str</code> <p>Region of the LLM must match the bucket</p> required <code>job_name</code> <code>str</code> <p>Unique identifier for this batch job</p> required <code>role_arn</code> <code>str</code> <p>AWS IAM role ARN with permissions for Bedrock and S3 access</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If AWS_PROFILE environment variable is not set</p> <code>ValueError</code> <p>If the provided role_arn doesn't exist or is invalid</p> Example <p>class PersonInfo(BaseModel): ...     name: str ...     age: int ... sbi = StructuredBatchInferer( ...     output_model=PersonInfo, ...     model_name=\"anthropic.claude-3-haiku-20240307-v1:0\", ...     bucket_name=\"my-inference-bucket\", ...     job_name=\"structured-batch-2024\", ...     role_arn=\"arn:aws:iam::123456789012:role/BedrockBatchRole\" ... )</p> Note <ul> <li>Converts the Pydantic model into a tool definition for the LLM</li> <li>All results will be validated against the provided schema</li> <li>Failed schema validations will raise errors during result processing</li> <li>Inherits all base BatchInferer functionality</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def __init__(\n    self,\n    output_model: BaseModel,\n    model_name: str,  # this should be an enum...\n    region: str,\n    bucket_name: str,\n    job_name: str,\n    role_arn: str,\n):\n    \"\"\"Initialize a StructuredBatchInferer for schema-validated batch processing.\n\n    Creates a batch inference manager that enforces structured outputs using\n    a Pydantic model schema. Automatically configures the model to use tools\n    for enforcing the output structure.\n\n    Args:\n        output_model (BaseModel): Pydantic model class defining the expected output structure\n        model_name (str): The AWS Bedrock model identifier\n        bucket_name (str): Name of the S3 bucket for storing job inputs and outputs\n        region (str): Region of the LLM must match the bucket\n        job_name (str): Unique identifier for this batch job\n        role_arn (str): AWS IAM role ARN with permissions for Bedrock and S3 access\n\n    Raises:\n        KeyError: If AWS_PROFILE environment variable is not set\n        ValueError: If the provided role_arn doesn't exist or is invalid\n\n    Example:\n        &gt;&gt;&gt; class PersonInfo(BaseModel):\n        ...     name: str\n        ...     age: int\n        ...\n        &gt;&gt;&gt; sbi = StructuredBatchInferer(\n        ...     output_model=PersonInfo,\n        ...     model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n        ...     bucket_name=\"my-inference-bucket\",\n        ...     job_name=\"structured-batch-2024\",\n        ...     role_arn=\"arn:aws:iam::123456789012:role/BedrockBatchRole\"\n        ... )\n\n    Note:\n        - Converts the Pydantic model into a tool definition for the LLM\n        - All results will be validated against the provided schema\n        - Failed schema validations will raise errors during result processing\n        - Inherits all base BatchInferer functionality\n    \"\"\"\n    self.output_model = output_model\n    self.tool = self._build_tool()\n    self.logger.info(\n        f\"Initialized StructuredBatchInferer with {output_model.__name__} schema\"\n    )\n    super().__init__(model_name, bucket_name, job_name, role_arn, region)\n</code></pre>"},{"location":"api/#llmbo.llmbo.StructuredBatchInferer.load_results","title":"<code>load_results()</code>","text":"<p>Load and validate batch inference results against the output schema.</p> <p>Reads the output files downloaded from S3 and validates each result against the Pydantic output_model specified during initialization. Populates:     - self.results: Raw inference results from the output JSONL file     - self.manifest: Statistics about the job execution     - self.instances: List of validated Pydantic model instances</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If either the results or manifest files are not found locally</p> <code>ValueError</code> <p>If any result fails schema validation or tool use validation</p> Note <ul> <li>Must call download_results() before calling this method</li> <li>All results must conform to the specified output_model schema</li> <li>Results must show successful tool use</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def load_results(self):\n    \"\"\"Load and validate batch inference results against the output schema.\n\n    Reads the output files downloaded from S3 and validates each result against\n    the Pydantic output_model specified during initialization. Populates:\n        - self.results: Raw inference results from the output JSONL file\n        - self.manifest: Statistics about the job execution\n        - self.instances: List of validated Pydantic model instances\n\n    Raises:\n        FileExistsError: If either the results or manifest files are not found locally\n        ValueError: If any result fails schema validation or tool use validation\n\n    Note:\n        - Must call download_results() before calling this method\n        - All results must conform to the specified output_model schema\n        - Results must show successful tool use\n    \"\"\"\n    super().load_results()\n    self.instances = [\n        self.validate_result(result[\"modelOutput\"]) for result in self.results\n    ]\n</code></pre>"},{"location":"api/#llmbo.llmbo.StructuredBatchInferer.prepare_requests","title":"<code>prepare_requests(inputs)</code>","text":"<p>Prepare structured batch inference requests with tool configurations.</p> <p>Extends the base preparation by adding tool definitions and tool choice parameters to each ModelInput. The tool definition is derived from the Pydantic output_model specified during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, ModelInput]</code> <p>Dictionary mapping record IDs to their corresponding ModelInput configurations. The record IDs will be used to track results.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If len(inputs) &lt; 100, as AWS Bedrock requires minimum batch size of 100</p> Example <p>class PersonInfo(BaseModel): ...     name: str ...     age: int sbi = StructuredBatchInferer(output_model=PersonInfo, ...) inputs = { ...     \"001\": ModelInput( ...         messages=[{\"role\": \"user\", \"content\": \"John is 25 years old\"}], ...     ) ... } sbi.prepare_requests(inputs)</p> Note <ul> <li>Automatically adds the output_model schema as a tool definition</li> <li>Sets tool_choice to force use of the defined schema</li> <li>Original ModelInputs are modified to include tool configurations</li> </ul> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def prepare_requests(self, inputs: Dict[str, ModelInput]):\n    \"\"\"Prepare structured batch inference requests with tool configurations.\n\n    Extends the base preparation by adding tool definitions and tool choice\n    parameters to each ModelInput. The tool definition is derived from the\n    Pydantic output_model specified during initialization.\n\n    Args:\n        inputs (Dict[str, ModelInput]): Dictionary mapping record IDs to their corresponding\n            ModelInput configurations. The record IDs will be used to track results.\n\n    Raises:\n        ValueError: If len(inputs) &lt; 100, as AWS Bedrock requires minimum batch size of 100\n\n    Example:\n        &gt;&gt;&gt; class PersonInfo(BaseModel):\n        ...     name: str\n        ...     age: int\n        &gt;&gt;&gt; sbi = StructuredBatchInferer(output_model=PersonInfo, ...)\n        &gt;&gt;&gt; inputs = {\n        ...     \"001\": ModelInput(\n        ...         messages=[{\"role\": \"user\", \"content\": \"John is 25 years old\"}],\n        ...     )\n        ... }\n        &gt;&gt;&gt; sbi.prepare_requests(inputs)\n\n    Note:\n        - Automatically adds the output_model schema as a tool definition\n        - Sets tool_choice to force use of the defined schema\n        - Original ModelInputs are modified to include tool configurations\n    \"\"\"\n    with_tools = {\n        id: self._add_tool_to_model_input(model_input)\n        for id, model_input in inputs.items()\n    }\n    super().prepare_requests(with_tools)\n</code></pre>"},{"location":"api/#llmbo.llmbo.StructuredBatchInferer.validate_result","title":"<code>validate_result(result)</code>","text":"<p>Validate and parse a single model output against the schema.</p> <p>Checks that the model used the specified tool correctly and validates the output against the Pydantic model schema.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>dict</code> <p>The raw model output containing content and metadata</p> required <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel</code> <p>An instance of the output_model containing the validated data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If: - Model didn't use the tool - Multiple tool uses were found - Output doesn't match schema</p> <code>TypeError</code> <p>If output data types don't match schema</p> Example <p>result = {\"stop_reason\": \"tool_use\", ...          \"content\": [{\"type\": \"tool_use\", ...                      \"input\": {\"name\": \"John\", \"age\": 30}}]} instance = sbi.validate_result(result) print(instance.name) 'John'</p> Source code in <code>src/llmbo/llmbo.py</code> <pre><code>def validate_result(\n    self,\n    result: dict,\n) -&gt; BaseModel:\n    \"\"\"Validate and parse a single model output against the schema.\n\n    Checks that the model used the specified tool correctly and validates\n    the output against the Pydantic model schema.\n\n    Args:\n        result (dict): The raw model output containing content and metadata\n\n    Returns:\n        BaseModel: An instance of the output_model containing the validated data\n\n    Raises:\n        ValueError: If:\n            - Model didn't use the tool\n            - Multiple tool uses were found\n            - Output doesn't match schema\n        TypeError: If output data types don't match schema\n\n    Example:\n        &gt;&gt;&gt; result = {\"stop_reason\": \"tool_use\",\n        ...          \"content\": [{\"type\": \"tool_use\",\n        ...                      \"input\": {\"name\": \"John\", \"age\": 30}}]}\n        &gt;&gt;&gt; instance = sbi.validate_result(result)\n        &gt;&gt;&gt; print(instance.name)\n        'John'\n    \"\"\"\n    if not result[\"stop_reason\"] == \"tool_use\":\n        self.logger.error(\"Model did not use tool\")\n        raise ValueError(\"Model did not use tool\")\n    if not len(result[\"content\"]) == 1:\n        self.logger.error(\"Multiple instances of tool use per execution\")\n        raise ValueError(\"Multiple instances of tool use per execution\")\n    if result[\"content\"][0][\"type\"] == \"tool_use\":\n        try:\n            output = self.output_model(**result[\"content\"][0][\"input\"])\n            return output\n        except TypeError as e:\n            self.logger.error(f\"Could not validate output {e}\")\n            raise ValueError(f\"Could not validate output {e}\")\n</code></pre>"}]}